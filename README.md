# RL_Learning
本项目基于Richard S. Sutton 以及Andrew G. Barto所著的《Reinforcement Learning-An Introduction Second Edition》，对该书的示例项目进行复现（见Example_Project），以进行RL的相关学习
# Chapter 1 导论
## 1.1 强化学习
动作往往影响的不仅仅是即时收益，也会影响下一个情境，从而影响随后的收益。这两个特征——**试错**和**延迟收益**——是强化学习两个最重要最显著的特征。  
智能体为了实现目标，而不断与环境产生交互。马尔可夫决策过程包含了这三个方面——**感知**、**动作**和**目标**。  

## 1.3 强化学习要素
除了智能体与环境之外，强化学习系统有4个核心要素：**策略**、**收益信号**、**价值函数**以及**对环境建立的模型**（可选）。  
**策略**定义了学习智能体在特定时间的行为方式。即：策略是环境状态（state）到动作（action）的映射，也是强化学习智能体的核心。  
**收益信号**定义了强化学习问题中的目标。一般来说，收益信号可能是环境状态和在此基础上所采取的动作的随机函数。  
收益信号表明了在短时间内什么是好的，而**价值函数**则表示了从长远的角度看什么是好的。一个状态的**价值**是一个智能体从这个状态开始，对将来积累的总收益的期望。  
而价值评估方法才是几乎所有强化学习算法中最重要的组成部分。  

## 1.4 局限性与适用范围
强化学习十分依赖“状态”这个概念，它既作为策略和价值函数的输入，又同时作为模型的输入与输出。

## 1.5 实例1 井字棋
